{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import the `REC0111.sav`, `RE223132.sav` and `RE516171.sav` files and their variables and values labels from this path `\"../../_data/endes/2019\"`. The name of imported files should be named as `rec_1`, `rec_2` and `rec_3` for files `REC0111.sav`, `RE223132.sav` and `RE516171.sav` respectively. The name of the variable and value labels should be `var_labels1` and `value_labels1` for `rec1`, `var_labels2` and `value_labels2` for `rec2`, and `var_labels3` and `value_labels3` for `rec3`. **Hint: See the section 3.3.4 of [the lecture 3](https://github.com/alexanderquispe/Diplomado_PUCP/blob/main/Lecture_3/Lecture_3.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we import the three datasets from the given path\n",
    "rec_1 = pd.read_spss(r\"../../_data/endes/2019/REC0111.sav\")\n",
    "rec_2 = pd.read_spss(r\"../../_data/endes/2019/RE223132.sav\")\n",
    "rec_3 = pd.read_spss(r\"../../_data/endes/2019/RE516171.sav\")\n",
    "\n",
    "# Now, in order to get the labels, we can use savReaderWriter for each of the three datasets:\n",
    "import savReaderWriter as sav\n",
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels1 = metadata.varLabels\n",
    "    value_labels1 = metadata.valueLabels\n",
    "\n",
    "# Now we apply the variable and value labels to the imported dataset\n",
    "rec_1.attrs[ 'value_labels' ] = value_labels1\n",
    "rec_1.attrs[ 'var_labels' ] = var_labels1\n",
    "\n",
    "# Now we replicate the same steps for rec_2\n",
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/RE223132.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels2 = metadata.varLabels\n",
    "    value_labels2 = metadata.valueLabels\n",
    "\n",
    "rec_2.attrs[ 'value_labels' ] = value_labels2\n",
    "rec_2.attrs[ 'var_labels' ] = var_labels2\n",
    "\n",
    "# Now we replicate the same steps for rec_3\n",
    "with sav.SavHeaderReader( r\"../../_data/endes/2019/RE516171.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "    var_labels3 = metadata.varLabels\n",
    "    value_labels3 = metadata.valueLabels\n",
    "\n",
    "rec_3.attrs[ 'value_labels' ] = value_labels3\n",
    "rec_3.attrs[ 'var_labels' ] = var_labels3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the following columns for each data set:\n",
    "|Data|Columns|\n",
    "|---|---|\n",
    "|rec1| CASEID, V000, V001, V002, V003, V004, V007, V008, V009, V010, V011, V012, V024, V102, V120, V121, V122, V123, V124, V125, V127, V133 |\n",
    "|rec2| CASEID, V201, V218, V301, V302, V323, V323A, V325A, V326, V327, V337, V359, V360, V361, V362, V363, V364, V367, V372, V372A, V375A, V376, V376A, V379, V380 |\n",
    "|rec3| CASEID, V501, V502, V503, V504, V505, V506, V507, V508, V509, V510, V511, V512, V513, V525, V613, V714, V715 |\n",
    "\n",
    "\n",
    "Additioanlly, you should update the variables and value labels objects. They must have information only for the selected columns. The new dataframes should be name as `rec1_1`, `rec2_1`, and `rec3_1`. The new varible labels objects should be named as `new_var_labels1`, `new_var_labels2`, and `new_var_labels3`. The new value labels objects should be named as `new_value_labels1`, `new_value_labels2`, and `new_value_labels3` **Hint: Use the `loc` and column names to filter. Update the dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FWe use the loc method in order to get the desired variables from the dataset for the three data sets\n",
    "rec1_1 = rec_1.loc[ : , [\"CASEID\", \"V000\", \"V001\", \"V002\", \"V003\", \"V004\", \"V007\", \"V008\", \"V009\", \"V010\", \"V011\", \"V012\", \"V024\", \"V102\", \"V120\", \"V121\", \"V122\", \"V123\", \"V124\", \"V125\", \"V127\", \"V133\" ]]\n",
    "\n",
    "rec2_1 = rec_2.loc[ : , ['CASEID', 'V201', 'V218', 'V301', 'V302', 'V323', 'V323A', 'V325A', 'V326', 'V327', 'V337', 'V359', 'V360', 'V361', 'V362', 'V363', 'V364', 'V367', 'V372', 'V372A', 'V375A', 'V376', 'V376A', 'V379', 'V380']]\n",
    "\n",
    "rec3_1 = rec_3.loc[ : , ['CASEID', 'V501', 'V502', 'V503', 'V504', 'V505', 'V506', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513', 'V525', 'V613', 'V714', 'V715']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate a new column for `rec1_1` named as `year`. It should be equal to `2019`. Also, you must update this new variable for the `var_labels` dictionary. Generate a new key for `new_var_labels1` and the value for this key should be **\"Year of the survey\"** **Hint: Use `loc` and `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we generated a variable that contains the year 2019 for all observations\n",
    "rec1_1[\"year\"] = 2019\n",
    "\n",
    "# We update the variable labels, using the update method\n",
    "var_labels1.update( {\"new_var_labels1\": \"Year of the survey\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge `rec1_1`, `rec2_1`, and `rec3_1` using **CASEID**. Name this new object as `endes_2019`. **Hint: Use [this link](https://stackoverflow.com/questions/53645882/pandas-merging-101)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we validate if CASEID it is unique within each data set\n",
    "(rec1_1['CASEID'].astype(str)).is_unique == (rec2_1['CASEID'].astype(str)).is_unique == (rec3_1['CASEID'].astype(str)).is_unique\n",
    "\n",
    "# Now we merge the 3 data sets using the merge method using CASEID as the id variable\n",
    "endes_2019 = rec1_1.merge( rec2_1, on =['CASEID'] , how = \"left\", validate = \"1:1\")\n",
    "endes_2019 = endes_2019.merge( rec3_1, on =['CASEID'] , how = \"left\", validate = \"1:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unify all the `new_var_labels` in one object and `new_value_labels` in another one object. Name these two objects as `var_labels` and `value_labels`. Use them to generate new attributes for `endes_2019`. These attributes should be named as `var_labels` and `value_labels`. **Hint: Use `update` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To unify all the variable labels and value labels, we need to use the update method\n",
    "var_labels1.update(var_labels2)\n",
    "var_labels1.update(var_labels3)\n",
    "\n",
    "# Now we generate a new dictionary with all the variable labels\n",
    "var_labels = var_labels1\n",
    "\n",
    "# Now we replicate the same process with the value labels\n",
    "value_labels1.update(value_labels2)\n",
    "value_labels1.update(value_labels3)\n",
    "value_labels = value_labels1\n",
    "\n",
    "# Finally we use attrs method to apply the labels to\n",
    "endes_2019.attrs['var_labels'] = var_labels\n",
    "endes_2019.attrs['value_labels'] = value_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Get the min, max, sd, n_obs, n_missing for the following columns **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)**. We want a dataframe with the following columns **Variables, Min, Max, Mean, N_obs, N_missing** and sort by the number of missing rows. **Hint: Use `describe` and `pivot` methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# En la V613 hay strings \"Respuesta no numérica\" asociados al valor 96\n",
    "# En la V715 hay strings \"No sabe\" asociados al valor 98\n",
    "\n",
    "# First we change some strings values according to the dictionary\n",
    "endes_2019.V613.replace('Respuesta no numérica', 96, inplace=True)\n",
    "endes_2019.V715.replace('No sabe', 98, inplace=True)\n",
    "\n",
    "# Now we change the class of the values for those variables in order to make them float (to summarize it)\n",
    "endes_2019['V613'] = endes_2019['V613'].astype( float )\n",
    "endes_2019['V715'] = endes_2019['V613'].astype( float )\n",
    "\n",
    "# Now we generate the table with the describe method for the selected variables and then transpose it to make it to the asked form\n",
    "table = endes_2019.loc[: , ['V201' , 'V511', 'V613' , 'V715' ]].describe()\n",
    "table = table.T\n",
    "\n",
    "# We transform the table with only the desired statistics\n",
    "table = table.loc[ : , ['Variables' , 'min', 'max' , 'mean' , 'count']]\n",
    "\n",
    "#\n",
    "table[\"N_missing\"] = list(endes_2019.loc[:,['V201' , 'V511', 'V613' , 'V715']].isnull().sum())\n",
    "table = table.rename(columns= {'mean' : \"Mean\" , 'min': \"Min\" , 'max' : \"Max\" , 'count' : \"N_obs\"}\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use `endes_2019` data to generate a new object named `mean_key_vars` to find the mean of **total children ever born (V201)**, **Ideal number of children (V613)**, **Husbands education-single yrs (V715)**, and **Age at first marriage (V511)** by year and department **(V024)**. Name these columns as **mean_total_children, mean_ideal_children, mean_hb_yr_educ and mean_first_marriage**, respectively. **Hint: Use groupby and [this link](https://stackoverflow.com/questions/40901770/is-there-a-simple-way-to-change-a-column-of-yes-no-to-1-0-in-a-pandas-dataframe).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using gropby to sort by year and department\n",
    "mean_key_vars.groupby( ['year', 'V024' ] )[['V201', 'V613', 'V715', 'v511']].mean().rename(columns = {'V201' : \"mean_total_childern\" , 'V613': \"mean_ideal_children\" , 'V715': \"mean_hb_yr_educ\", 'V511': \"mean_first_marriage\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reshape `mean_key_vars` from wide to long. We want a dataframe with three columns **dpto, variables, values**. Name this object as `reshape_mean_key_vars`. **Hint: Use melt method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using melt method to generate a dataframe with the three columns asked\n",
    "reshape_mean_key_vars = pd.melt( mean_key_vars, id_vars [ \"dpto\", \"variables\", \"values\" ], var_name=\"dpto\", var_name=\"variables\", value_name=\"values\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Replicate your answers for questions 7 and 8, but in one line of code. Make it the most simple as possible. **NO HINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to get all in one line, we need to apply the melt method to the database generated in question 7. The rest is the same as coded before\n",
    "reshape_mean_key_vars2 = pd.melt(endes_2019.loc[ : , ['V024' ,'year' , 'V201', 'V613' , 'V715' , 'V511']].groupby( [ 'V024' ,'year' ] ).mean().rename(columns = {'V201' : \"mean_total_childern\" , 'V613': \"mean_ideal_children\" , 'V715': \"mean_hb_yr_educ\", 'V511': \"mean_first_marriage\"}).reset_index(), id_vars = ['V024' ,'year'] , value_vars= [\"mean_total_childern\", \"mean_ideal_children\", \"mean_hb_yr_educ\" , \"mean_first_marriage\"] )\n",
    "reshape_mean_key_vars2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Merge `reshape_mean_key_vars` with `endes_2019`. Name this object `final_result`. **Hint: Use merge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we need to merge the endes_2019 dataset with the reshape_mean_key_vars2 with m:m form.\n",
    "final_result = endes_2019.merge( reshape_mean_key_vars2, on =['V024', 'year'] , how = \"left\", validate = \"m:m\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
