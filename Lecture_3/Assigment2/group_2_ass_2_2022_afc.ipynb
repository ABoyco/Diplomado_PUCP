{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Assignment\n",
    "\n",
    "It is totally prohibited to use any kind of loop. You can use stackoverflow. If you copy codes from previous answers, explain each step. No explanation is `0 points`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dictionaries\n",
    "1. Create a dictionary with two keys: `even_numbers` and `odd_numbers`. The first key should have all the even numbers in this range `[0, 2000]`, and the second key must have all the odd numbers in this range `[9000, 19000]`. The values should be stored in a `list`. **Hint: Use the `np.arange`, `zip`, and `np.tolist()` functions.** <br><br>\n",
    "2. Print the value of `brand` key of `car` dictionary. **Hint: Use the `get` method.** <br><br>\n",
    "3. Print all the the values of `brand` key of `car` dictionary. **Hint: Use the `values` method.** <br><br>\n",
    "4. Print the max value  of `friday` in `january` of `hr_sleep` dictionary. **Hint: [Indexing in nested dictioanries](https://stackoverflow.com/questions/25836376/how-to-get-the-inner-indexes-of-a-nested-dictionary-in-python).** <br><br>\n",
    "5. Add `march` key to the `hr_sleep` dictionary using `week1` and `values2` Python lists. **Hint: Use `zip` function adn [this link](https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary).** <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = {\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}\n",
    "\n",
    "hr_sleep = {\"january\": {\"wednesday\": 7,\n",
    "                      \"thursday\": 8,\n",
    "                      \"friday\": [2, 2, 1, 2]},\n",
    "          \"february\": {\"saturday\": 5,\n",
    "                       \"sunday\": 10,\n",
    "                       \"monday\": 8\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1 = ['monday', 'sunday']\n",
    "values2 = [ [2, 3, 4 ] , 8, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ford'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pregunta 2\n",
    "car.get(\"brand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Ford', 'Mustang', 1964])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pregunta 3\n",
    "car.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Numpy\n",
    "1. Replace all the `even_numbers` in `np1` with 100. **Hint: Use `indexing` in arrys and [this filer](https://stackoverflow.com/questions/41638751/filtering-even-numbers-in-python).** <br><br>\n",
    "2. Create a 3x3 matrix with values ranging from 0 to 8. **Hint: Use `np.arange` and `np.reshape` method.** <br><br>\n",
    "3. Consider an array `Z = [1,2,3,4,5,6,7,8,9,10]`, generate an array `R = [ [ 1, 2, 3, 4], [ 2, 3, 4, 5 ], [ 3, 4, 5, 6 ], ..., [ 7, 8, 9, 10 ] ]`?.**Hint: Use `np.arange` and `np.concatenate`** <br><br>\n",
    "4. Create a 3x3x3 array with random values. **Hint: Use [`np.random.random`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html).** <br><br>\n",
    "5. Comment why this expression `np.nan == np.nan` is False. **Hint: Use stackoverflow.** <br><br>\n",
    "6. Print the `mean` and `standard deviation` of `np2`. **Hint: Use `f-string`, `np.mean`, and `np.var` methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np1 = np.arange( 200, 750)\n",
    "np2 = np.random.normal( 50 , 4, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Restricted Least Squares\n",
    "\n",
    "Given the following equation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "lnCT &= \\beta_{0}+\\beta_{q}lnq+ \\frac{1}{2}\\beta_{qq} lnq^2+\\beta_{q1}lnqlnp_1+\\beta_{q2}lnqlnp_2+ \\beta_{q3}lnqlnp_{3} +\\beta_{1}lnp_1+\\beta_{2}lnp_2+ \\beta_{3}lnp_3 \\\\\n",
    "& + \\frac{1}{2}\\beta_{11}ln^{2}p_1+ \\frac{1}{2}\\beta_{22}ln^{2}p_{2}+ \\frac{1}{2}\\beta_{33}ln^{2}p_{3} + \\frac{1}{2}\\beta_{12}lnp_{1}lnp_{2}+ \\frac{1}{2}\\beta_{13}lnp_{1}lnp_{3}+\\frac{1}{2}\\beta_{23}lnp_{2}lnp_{3} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "ST: \n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\beta_{1} + \\beta_{2} + \\beta_{3} &= 1 \\\\\n",
    "\\beta_{q1} + \\beta_{q2} + \\beta_{q3} &= 0 \\\\\n",
    "\\beta_{11} + \\beta_{12} + \\beta_{13} &= 0 \\\\\n",
    "\\beta_{21} + \\beta_{22} + \\beta_{23} &= 0 \\\\\n",
    "\\beta_{31} + \\beta_{32} + \\beta_{33} &= 0 \\\\\n",
    "\\beta_{ij} = \\beta_{ji}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Get $\\widehat{\\boldsymbol{\\beta}}^{(RLS)}$ vector from the equation bellow. Use the `q`, `p1`, `p2`, `p3`, `CT` numpies. <br><br>\n",
    "2. Get the `covariance matrix` $\\mathbb{V}{\\rm ar} \\left(\\widehat{\\boldsymbol{\\beta}}^{(RLS)} \\right)$. <br><br>\n",
    "**Hint: For more information about Restricted Least Squares [click here](http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-4-Multiple-RLS.html).**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get ùú∑ÀÜ(ùëÖùêøùëÜ) vector from the equation bellow. Use the q, p1, p2, p3, CT numpies.\n",
    "\n",
    "# First we import the relevant libraries:\n",
    "\n",
    "import numpy as np\n",
    "from math import log\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we import the dataset considering the proposed name \"greene\"\n",
    "greene = pd.read_csv(r\"/Users/joselinchavez/Documents/GitHub/Diplomado_PUCP/Lecture_3/christensen_greene_f4.csv\")\n",
    "\n",
    "#In this case we use the path to the folder where the database \n",
    "#is stored because it couldn't be taken directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>COST</th>\n",
       "      <th>Q</th>\n",
       "      <th>PL</th>\n",
       "      <th>SL</th>\n",
       "      <th>PK</th>\n",
       "      <th>SK</th>\n",
       "      <th>PF</th>\n",
       "      <th>SF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.0</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>127.208861</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>53.269962</td>\n",
       "      <td>10469.410759</td>\n",
       "      <td>8001.863228</td>\n",
       "      <td>0.138972</td>\n",
       "      <td>71.421063</td>\n",
       "      <td>0.226387</td>\n",
       "      <td>30.745785</td>\n",
       "      <td>0.632355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60.792333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.059326</td>\n",
       "      <td>15187.520802</td>\n",
       "      <td>1398.837478</td>\n",
       "      <td>0.054735</td>\n",
       "      <td>11.977489</td>\n",
       "      <td>0.060836</td>\n",
       "      <td>7.943089</td>\n",
       "      <td>0.083324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5063.490000</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>31.725000</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>78.500000</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>10.221050</td>\n",
       "      <td>1971.000000</td>\n",
       "      <td>6975.177500</td>\n",
       "      <td>0.099725</td>\n",
       "      <td>67.605000</td>\n",
       "      <td>0.192550</td>\n",
       "      <td>24.477350</td>\n",
       "      <td>0.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>138.500000</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>25.545400</td>\n",
       "      <td>5645.500000</td>\n",
       "      <td>7890.185000</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>74.120000</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>30.657900</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>178.750000</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>55.315900</td>\n",
       "      <td>12365.750000</td>\n",
       "      <td>8855.230000</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>78.794250</td>\n",
       "      <td>0.252775</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>218.000000</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>737.408800</td>\n",
       "      <td>115500.000000</td>\n",
       "      <td>13044.000000</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>92.650000</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>51.463000</td>\n",
       "      <td>0.813600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id    YEAR        COST              Q            PL  \\\n",
       "count  158.000000   158.0  158.000000     158.000000    158.000000   \n",
       "mean   127.208861  1970.0   53.269962   10469.410759   8001.863228   \n",
       "std     60.792333     0.0   87.059326   15187.520802   1398.837478   \n",
       "min      1.000000  1970.0    0.130400       4.000000   5063.490000   \n",
       "25%     78.500000  1970.0   10.221050    1971.000000   6975.177500   \n",
       "50%    138.500000  1970.0   25.545400    5645.500000   7890.185000   \n",
       "75%    178.750000  1970.0   55.315900   12365.750000   8855.230000   \n",
       "max    218.000000  1970.0  737.408800  115500.000000  13044.000000   \n",
       "\n",
       "               SL          PK          SK          PF         SF   \n",
       "count  158.000000  158.000000  158.000000  158.000000  158.000000  \n",
       "mean     0.138972   71.421063    0.226387   30.745785    0.632355  \n",
       "std      0.054735   11.977489    0.060836    7.943089    0.083324  \n",
       "min      0.045900   31.725000    0.092400    9.000000    0.243500  \n",
       "25%      0.099725   67.605000    0.192550   24.477350    0.590100  \n",
       "50%      0.123100   74.120000    0.218600   30.657900    0.645000  \n",
       "75%      0.169800   78.794250    0.252775   36.000000    0.686900  \n",
       "max      0.329100   92.650000    0.457100   51.463000    0.813600  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can check the database\n",
    "greene.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the provided renaming:\n",
    "ct = greene.COST.values\n",
    "q = greene.Q.values\n",
    "p1 = greene.PL.values\n",
    "p2 = greene.PF.values\n",
    "p3 = greene.PK.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also took logs of the relevant variables (14 variables \n",
    "# without the intercept, independents) and we can give it the same form as \n",
    "#requested by proposed equation \n",
    "\n",
    "ln_q = np.log(q)\n",
    "ln2_q = 0.5*np.square(ln_q) # in this case we need the additional \n",
    "# square to logarithm\n",
    "\n",
    "ln_p1 = np.log(p1)\n",
    "ln_p2 = np.log(p2)\n",
    "ln_p3 = np.log(p3)\n",
    "\n",
    "ln_qp1 = np.multiply(ln_q, ln_p1) # in this case we need to multiply\n",
    "# adittional to logarithm\n",
    "ln_qp2 = np.multiply(ln_q, ln_p2)\n",
    "ln_qp3 = np.multiply(ln_q, ln_p3)\n",
    "\n",
    "ln2_p1 = 0.5*np.square(ln_p1)\n",
    "ln2_p2 = 0.5*np.square(ln_p2)\n",
    "ln2_p3 = 0.5*np.square(ln_p3)\n",
    "\n",
    "ln_p1p2 = np.multiply(ln_p1, ln_p2)\n",
    "ln_p1p3 = np.multiply(ln_p1, ln_p3)\n",
    "ln_p2p3 = np.multiply(ln_p2, ln_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the intercept with the function ones:\n",
    "intercept = np.ones((len(q), 1))\n",
    "#intercept\n",
    "\n",
    "# Likewise, we take logarithm of the dependent variable:\n",
    "ln_ct = np.log(ct)\n",
    "#ln_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  2.07944154,  2.16203856, ..., 25.53597848,\n",
       "        36.87257416, 12.06308434],\n",
       "       [ 1.        ,  6.76734313, 22.89846649, ..., 27.52922179,\n",
       "        38.14391527, 12.86998372],\n",
       "       [ 1.        ,  7.25276242, 26.30128135, ..., 33.47177703,\n",
       "        33.28867699, 13.81021534],\n",
       "       ...,\n",
       "       [ 1.        ,  5.96870756, 17.81273497, ..., 32.93311772,\n",
       "        37.80502354, 16.56284231],\n",
       "       [ 1.        ,  8.57866451, 36.79674242, ..., 34.23242826,\n",
       "        39.94379007, 16.26769248],\n",
       "       [ 1.        ,  9.63036563, 46.3719711 , ..., 28.72378963,\n",
       "        38.10118094, 13.96783596]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We must also create the big X matrix, it must contain the 14 \n",
    "# variables and the intercept, so we use the function stack for columns\n",
    "\n",
    "X_big = np.column_stack([intercept, ln_q, ln2_q, ln_qp1, ln_qp2, \n",
    "                         ln_qp3, ln_p1, ln_p2, ln_p3, ln2_p1, ln2_p2,\n",
    "                         ln2_p3, ln_p1p2, ln_p1p3, ln_p2p3])\n",
    "\n",
    "X_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 1 1]]\n",
      "[1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# To estimate the ùú∑ÀÜ(ùëÖùêøùëÜ) we should introduce the restrictions in a matrix called L \n",
    "# (similar to the hint). Actually, we can verify that some betas doesn't exist for the\n",
    "# third, fourth, and fifth restrictions, so the last constraint is implicitly introduced on these \n",
    "\n",
    "L = np.array( ( [ 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0 ],\n",
    "                [ 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ], \n",
    "                [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0 ], \n",
    "                [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1 ], \n",
    "                [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1 ] ) )\n",
    "\n",
    "rr = [ 1, 0, 0, 0, 0 ]\n",
    "\n",
    "print(L)\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implementing the total database:\n",
    "data_smpl = pd.DataFrame(np.column_stack([ln_ct, X_big]), columns = [\"ln_ct\",\"intercept\", \"ln_q\", \"ln2_q\", \n",
    "                                                                     \"ln_qp1\", \"ln_qp2\", \"ln_qp3\", \"ln_p1\", \n",
    "                                                                     \"ln_p2\", \"ln_p3\", \"ln2_p1\", \"ln2_p2\",\n",
    "                                                                     \"ln2_p3\", \"ln_p1p2\", \"ln_p1p3\", \"ln_p2p3\"])\n",
    "#data_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "intercept -7.111200\n",
      "ln_q       0.461368\n",
      "ln2_q      0.059955\n",
      "ln_qp1    -0.004354\n",
      "ln_qp2     0.036778\n",
      "ln_qp3    -0.032424\n",
      "ln_p1      0.215566\n",
      "ln_p2      0.402923\n",
      "ln_p3      0.381511\n",
      "ln2_p1    -0.004356\n",
      "ln2_p2     0.002578\n",
      "ln2_p3    -0.007651\n",
      "ln_p1p2   -0.002936\n",
      "ln_p1p3    0.007292\n",
      "ln_p2p3    0.000358\n"
     ]
    }
   ],
   "source": [
    "# Now we calculate the usual BETA_ols\n",
    "XTX_inv  = np.linalg.inv(np.dot(np.transpose(X_big), X_big))\n",
    "BETAS_ols = np.dot(XTX_inv, np.dot(np.transpose(X_big), ln_ct))\n",
    "\n",
    "#Nevertheless, we need to calculate the RLS estimator expressed as Œ≤_RLS = Œ≤_OLS + \"Restriction Adjustment\"\n",
    "RA_1 = np.dot(XTX_inv, np.transpose(L))\n",
    "RA_2 = np.linalg.inv(np.linalg.multi_dot([L, XTX_inv, np.transpose(L)]))\n",
    "RA_3 = np.dot(L, BETAS_ols) - np.array(rr)\n",
    "RA   = RA_1.dot(RA_2).dot(RA_3)\n",
    "\n",
    "# With this adjustment component, we can finally calculate the RLS estimator:\n",
    "BETAS_rls = BETAS_ols - RA\n",
    "print(pd.DataFrame(BETAS_rls, index = [\"intercept\", \"ln_q\", \"ln2_q\", \"ln_qp1\", \"ln_qp2\", \"ln_qp3\", \"ln_p1\", \n",
    "                                \"ln_p2\", \"ln_p3\", \"ln2_p1\", \"ln2_p2\", \"ln2_p3\", \"ln_p1p2\", \"ln_p1p3\", \"ln_p2p3\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           intercept      ln_q     ln2_q    ln_qp1    ln_qp2    ln_qp3  \\\n",
      "intercept   8.485895 -0.096160  0.000206  0.015677 -0.022104  0.006427   \n",
      "ln_q       -0.096160  0.016015 -0.000070 -0.002912  0.002270  0.000642   \n",
      "ln2_q       0.000206 -0.000070  0.000020 -0.000016 -0.000002  0.000019   \n",
      "ln_qp1      0.015677 -0.002912 -0.000016  0.000585 -0.000363 -0.000222   \n",
      "ln_qp2     -0.022104  0.002270 -0.000002 -0.000363  0.000654 -0.000291   \n",
      "ln_qp3      0.006427  0.000642  0.000019 -0.000222 -0.000291  0.000513   \n",
      "ln_p1      -3.050502  0.012193 -0.000038 -0.001377  0.005099 -0.003722   \n",
      "ln_p2       1.613544 -0.014739  0.000150  0.001701 -0.006121  0.004419   \n",
      "ln_p3       1.436958  0.002546 -0.000112 -0.000324  0.001022 -0.000697   \n",
      "ln2_p1      0.198259  0.000797  0.000012 -0.000240 -0.000166  0.000406   \n",
      "ln2_p2     -0.045968 -0.001459  0.000031  0.000281  0.000029 -0.000310   \n",
      "ln2_p3     -0.411072 -0.000658 -0.000048  0.000287  0.000230 -0.000517   \n",
      "ln_p1p2    -0.281682  0.000002 -0.000046  0.000123  0.000184 -0.000307   \n",
      "ln_p1p3     0.083423 -0.000799  0.000033  0.000117 -0.000018 -0.000099   \n",
      "ln_p2p3     0.327650  0.001457  0.000015 -0.000404 -0.000213  0.000617   \n",
      "\n",
      "              ln_p1     ln_p2     ln_p3    ln2_p1    ln2_p2    ln2_p3  \\\n",
      "intercept -3.050502  1.613544  1.436958  0.198259 -0.045968 -0.411072   \n",
      "ln_q       0.012193 -0.014739  0.002546  0.000797 -0.001459 -0.000658   \n",
      "ln2_q     -0.000038  0.000150 -0.000112  0.000012  0.000031 -0.000048   \n",
      "ln_qp1    -0.001377  0.001701 -0.000324 -0.000240  0.000281  0.000287   \n",
      "ln_qp2     0.005099 -0.006121  0.001022 -0.000166  0.000029  0.000230   \n",
      "ln_qp3    -0.003722  0.004419 -0.000697  0.000406 -0.000310 -0.000517   \n",
      "ln_p1      1.151568 -0.589638 -0.561930 -0.078231  0.022798  0.154840   \n",
      "ln_p2     -0.589638  0.343127  0.246511  0.039021 -0.001273 -0.085612   \n",
      "ln_p3     -0.561930  0.246511  0.315419  0.039210 -0.021524 -0.069228   \n",
      "ln2_p1    -0.078231  0.039021  0.039210  0.005550 -0.002064 -0.010488   \n",
      "ln2_p2     0.022798 -0.001273 -0.021524 -0.002064  0.007181 -0.001587   \n",
      "ln2_p3     0.154840 -0.085612 -0.069228 -0.010488 -0.001587  0.025193   \n",
      "ln_p1p2    0.105136 -0.061679 -0.043457 -0.006987 -0.003352  0.018634   \n",
      "ln_p1p3   -0.026906  0.022659  0.004247  0.001437  0.005416 -0.008146   \n",
      "ln_p2p3   -0.127934  0.062953  0.064981  0.009051 -0.003828 -0.017047   \n",
      "\n",
      "            ln_p1p2   ln_p1p3   ln_p2p3  \n",
      "intercept -0.281682  0.083423  0.327650  \n",
      "ln_q       0.000002 -0.000799  0.001457  \n",
      "ln2_q     -0.000046  0.000033  0.000015  \n",
      "ln_qp1     0.000123  0.000117 -0.000404  \n",
      "ln_qp2     0.000184 -0.000018 -0.000213  \n",
      "ln_qp3    -0.000307 -0.000099  0.000617  \n",
      "ln_p1      0.105136 -0.026906 -0.127934  \n",
      "ln_p2     -0.061679  0.022659  0.062953  \n",
      "ln_p3     -0.043457  0.004247  0.064981  \n",
      "ln2_p1    -0.006987  0.001437  0.009051  \n",
      "ln2_p2    -0.003352  0.005416 -0.003828  \n",
      "ln2_p3     0.018634 -0.008146 -0.017047  \n",
      "ln_p1p2    0.014487 -0.007500 -0.011135  \n",
      "ln_p1p3   -0.007500  0.006063  0.002084  \n",
      "ln_p2p3   -0.011135  0.002084  0.014963  \n"
     ]
    }
   ],
   "source": [
    "# 2. Get the covariance matrix ùïçar(ùú∑ÀÜ(ùëÖùêøùëÜ))\n",
    "\n",
    "# To calculate the covariance matrix we create the estimation residuals, \n",
    "# then we find the covvar matrix called \"VarCov_rls\":\n",
    "ln_ct_fit = np.dot(X_big, BETAS_rls)\n",
    "e_resid = np.array(ln_ct) - ln_ct_fit\n",
    "\n",
    "sigma2_rls = np.sum(e_resid**2) / (len(data_smpl.index) - (len(BETAS_rls) - len(rr)))\n",
    "\n",
    "D_mat = np.identity(len(BETAS_rls)) - RA_1.dot(RA_2).dot(L)\n",
    "VarCov_rls = sigma2_rls * D_mat.dot(XTX_inv)\n",
    "\n",
    "print(pd.DataFrame(VarCov_rls, \n",
    "                   index = [\"intercept\", \"ln_q\", \"ln2_q\", \"ln_qp1\", \"ln_qp2\", \"ln_qp3\", \"ln_p1\", \n",
    "                                \"ln_p2\", \"ln_p3\", \"ln2_p1\", \"ln2_p2\", \"ln2_p3\", \"ln_p1p2\", \"ln_p1p3\", \"ln_p2p3\"], \n",
    "                   columns = [\"intercept\", \"ln_q\", \"ln2_q\", \"ln_qp1\", \"ln_qp2\", \"ln_qp3\", \"ln_p1\", \n",
    "                                \"ln_p2\", \"ln_p3\", \"ln2_p1\", \"ln2_p2\", \"ln2_p3\", \"ln_p1p2\", \"ln_p1p3\", \"ln_p2p3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code bellow uncomment the second line to install savReaderWriter library. The `dict_varlabels` has the labels of the columns of rec1.\n",
    "\n",
    "1. Check if CASEID identifies each observation uniquely. **[Hint: Use is_unique method.](https://pandas.pydata.org/docs/reference/api/pandas.Series.is_unique.html)**\n",
    "2. Make the CASEID column the index of your data set. **[Hint: Use set_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html)**. \n",
    "3. Keep women who are 15-30 years old and live in Urban areas. **[Hint: Use query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)**.\n",
    "4. Generate a new column with the month of born. Just use the first three letters.  Use the English names of the months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "1. Ejecute el c√≥digo a continuaci√≥n, descomente la segunda l√≠nea para instalar la biblioteca savReaderWriter. El dict_varlabels tiene las etiquetas de las columnas de rec1.\n",
    "\n",
    "2. Compruebe si CASEID identifica cada observaci√≥n de forma √∫nica. Sugerencia: utilice el m√©todo is_unique.\n",
    "\n",
    "3. Haga que la columna CASEID sea el √≠ndice de su conjunto de datos. Sugerencia: use set_index.\n",
    "\n",
    "4. Mantenga a las mujeres que tengan entre 15 y 30 a√±os y vivan en √°reas urbanas. Sugerencia: use la consulta.\n",
    "\n",
    "5. Genera una nueva columna con el mes de nacimiento. Solo usa las tres primeras letras. Usa los nombres en ingl√©s de los meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in c:\\anaconda\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\anaconda\\lib\\site-packages (from pyreadstat) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\anaconda\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyreadstat) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 1. Instalaci√≥n preliminar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "!pip install pyreadstat\n",
    "import savReaderWriter as sav\n",
    "rec1 = pd.read_spss( fr\"C:/Users/USER/Documents/GitHub/Diplomado_PUCP/_data/endes/2019/REC0111.sav\" )\n",
    "\n",
    "with sav.SavHeaderReader( fr\"C:/Users/USER/Documents/GitHub/Diplomado_PUCP/_data/endes/2019/REC0111.sav\", ioUtf8=True) as header:\n",
    "    metadata = header.all()\n",
    "\n",
    "dict_varlabels = metadata[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID1': 'A√±o',\n",
       " 'HHID': 'Identificaci√≥n Cuestionario del Hogar',\n",
       " 'CASEID': 'Identificaci√≥n Cuestionario Individual',\n",
       " 'V001': 'Conglomerado',\n",
       " 'V002': 'N√∫mero de vivienda',\n",
       " 'V003': 'N√∫mero de l√≠nea de entrevistada',\n",
       " 'V004': 'Unidad de √°rea final',\n",
       " 'V007': 'A√±o de la entrevista',\n",
       " 'V008': 'Fecha de la entrevista, Codificaci√≥n centenaria de meses (CMC)',\n",
       " 'V009': 'Mes de nacimiento de la entrevistada',\n",
       " 'V010': 'A√±o de nacimiento de la entrevistada',\n",
       " 'V011': 'Fecha de nacimiento, Codificaci√≥n centenaria de meses (CMC)',\n",
       " 'V012': 'Edad actual - entrevistada',\n",
       " 'V013': 'Edad actual por grupos de 5 a√±os',\n",
       " 'V014': 'Integridad de la informaci√≥n para la fecha de nacimiento ',\n",
       " 'V015': 'Resultado entrevista individual',\n",
       " 'V017': 'Inicio del calendario, Codificaci√≥n centenaria de mesesl CMC',\n",
       " 'V018': 'Columna del mes de la entrevista',\n",
       " 'V019': 'Duraci√≥n del calendario',\n",
       " 'V019A': 'N√∫mero de columnas de calendario',\n",
       " 'V020': 'Muestra alguna vez casada',\n",
       " 'V021': 'Unidad de muestreo primario - conglomerado',\n",
       " 'V023': 'Dominio de ejemplo - Departamento',\n",
       " 'V024': 'Regi√≥n',\n",
       " 'V025': 'Tipo de lugar de residencia',\n",
       " 'V026': 'El lugar de residencia en el que se entrevist√≥ - De Facto',\n",
       " 'V027': 'N√∫mero de visitas',\n",
       " 'V028': 'Identificaci√≥n del entrevistador',\n",
       " 'V029': 'Identificador del digitador',\n",
       " 'V030': 'Supervisor de campo',\n",
       " 'V031': 'Editor de campo',\n",
       " 'V032': 'Editor de la oficina',\n",
       " 'V033': 'Selecci√≥n final del √°rea de probabilidad',\n",
       " 'V034': 'N√∫mero de orden del esposo',\n",
       " 'V040': 'Altitud del conglomerado en metros',\n",
       " 'V042': 'Selecci√≥n de hogar para hemoglobina',\n",
       " 'V043': 'Selecci√≥n para m√≥dulo de estatus de mujeres',\n",
       " 'V044': 'Selecci√≥n para m√≥dulo de violencia domestica',\n",
       " 'V000': 'C√≥digo y fase del pa√≠s',\n",
       " 'Q105DD': 'Dia de nacimeinto de la entrevistada',\n",
       " 'V101': 'Regi√≥n',\n",
       " 'V102': 'Tipo de lugar de residencia',\n",
       " 'V103': 'Lugar de residencia de la infancia',\n",
       " 'V104': 'Cuanto tiempo tiene viviendo continuamente en el lugar de residencia actual',\n",
       " 'V105': 'Tipo de lugar de residencia anteriormente',\n",
       " 'V106': 'Nivel educativo m√°s alto',\n",
       " 'V107': 'A√±o/grado de educaci√≥n m√°s alto aprobado',\n",
       " 'V113': 'Fuente principal de abasteciemiento de agua potable que utilizan en su hogar para tomar o beber',\n",
       " 'V115': 'Tiempo para llegar a la fuente de agua',\n",
       " 'V116': 'Tipo de instalaci√≥n sanitaria',\n",
       " 'V119': 'En su hogar tiene: electricidad',\n",
       " 'V120': 'En su hogar tiene: radio',\n",
       " 'V121': 'En su hogar tiene: televisi√≥n',\n",
       " 'V122': 'En su hogar tiene: refrigerador',\n",
       " 'V123': 'En su hogar tiene: bicicleta',\n",
       " 'V124': 'En su hogar tiene: motocicleta/motocar',\n",
       " 'V125': 'En su hogar tiene: coche/cami√≥n',\n",
       " 'V127': 'Material predominante del piso de la vivienda',\n",
       " 'V128': 'Material predominante de las paredes exteriores de la vivienda',\n",
       " 'V129': 'Material predominante del techo de la vivienda',\n",
       " 'V130': 'Religi√≥n',\n",
       " 'V131': 'Etnicidad',\n",
       " 'V133': 'Educaci√≥n en a√±os simples',\n",
       " 'V134': 'El lugar en el que se realiz√≥ la entrevista  De-facto',\n",
       " 'V135': 'Residente habitual o visitante',\n",
       " 'V136': 'N√∫mero de miembros del hogar',\n",
       " 'V137': 'N√∫mero de ni√±os de 6 a√±os de edad ',\n",
       " 'V138': 'N√∫mero de mujeres de 15 a 49 a√±os de edad elegibles en el hogar ',\n",
       " 'V139': 'Regi√≥n, residencia habitual De-jure',\n",
       " 'V140': 'Tipo de √°rea de residencia De-jure',\n",
       " 'V141': 'Lugar de residencia De-jure',\n",
       " 'V149': 'Logro educativo',\n",
       " 'V150': 'Relaci√≥n con el jefe del hogar',\n",
       " 'V151': 'Sexo del Jefe del Hogar',\n",
       " 'V152': 'Edad del jefe del hogar',\n",
       " 'V153': 'En su hogar tiene: tel√©fono',\n",
       " 'AWFACTT': 'Factor todas las mujeres - total',\n",
       " 'AWFACTU': 'Factor todas las mujeres - urbano/rural',\n",
       " 'AWFACTR': 'Factor todas las mujeres - regional',\n",
       " 'AWFACTE': 'Factor todas las mujeres - educaci√≥n',\n",
       " 'AWFACTW': 'Factor todas las mujeres - √≠ndice de riqueza',\n",
       " 'V155': 'Alfabetizaci√≥n',\n",
       " 'V156': 'Alguna vez particip√≥ en un programa de alfabetizaci√≥n (no incluyendo la escuela primaria)',\n",
       " 'V157': 'Frecuencia de lectura de un peri√≥dico o revista',\n",
       " 'V158': 'Frecuencia de escuchar radio',\n",
       " 'V159': 'Frecuencia de ver televisi√≥n',\n",
       " 'V160': 'Ba√±o compartido con otros hogares',\n",
       " 'V161': 'Tipo de combustible para cocinar',\n",
       " 'V166': 'Resultados de la prueba del yodo en la sal',\n",
       " 'V167': 'N√∫mero de viajes en los √∫ltimos 12 meses',\n",
       " 'V168': 'Afuera m√°s de un mes en los √∫ltimos 12 meses',\n",
       " 'ML101': 'Tipo de mosquitero que utilizo para dormir √∫ltima noche',\n",
       " 'QD333_1': 'Alguna dificultad o limitaci√≥n permanente para ver, a√∫n usando anteojos',\n",
       " 'QD333_2': 'Alguna dificultad o limitaci√≥n permanente para oir, a√∫n usando aud√≠fonos',\n",
       " 'QD333_3': 'Alguna dificultad o limitaci√≥n permanente para hablar o comunicarse, a√∫n usando la lengua de se√±as u otro',\n",
       " 'QD333_4': 'Alguna dificultad o limitaci√≥n permanente para moverse o caminar para usar brazos y/o piernas',\n",
       " 'QD333_5': 'Alguna dificultad o limitaci√≥n permanente para entender o aprender (concentrarse y recordarse)',\n",
       " 'QD333_6': 'Alguna dificultad o limitaci√≥n permanente para relacionarse con los dem√°s, por sus pensamientos, sentimientos, emociones o conductas',\n",
       " 'UBIGEO': 'C√≥digo de Ubicaci√≥n Gegr√°fica',\n",
       " 'V022': 'Estratos',\n",
       " 'V005': 'Factor de ponderacion',\n",
       " 'V190': '√çndice de riqueza',\n",
       " 'V191': 'Factor de puntuaci√≥n del √≠ndice de riqueza (5 decimales)',\n",
       " 'mujeres12a49': 'Mujeres de 12 a 49 a√±os de edad',\n",
       " 'NCONGLOME': 'N√∫mero de Conglomerado (proveniente del marco)'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_varlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID1</th>\n",
       "      <th>HHID</th>\n",
       "      <th>CASEID</th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>...</th>\n",
       "      <th>QD333_4</th>\n",
       "      <th>QD333_5</th>\n",
       "      <th>QD333_6</th>\n",
       "      <th>UBIGEO</th>\n",
       "      <th>V022</th>\n",
       "      <th>V005</th>\n",
       "      <th>V190</th>\n",
       "      <th>V191</th>\n",
       "      <th>mujeres12a49</th>\n",
       "      <th>NCONGLOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>000100201</td>\n",
       "      <td>000100201  2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>010101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154803.0</td>\n",
       "      <td>Rico</td>\n",
       "      <td>1.234450</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>000100201</td>\n",
       "      <td>000100201  3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>010101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154803.0</td>\n",
       "      <td>Rico</td>\n",
       "      <td>1.234450</td>\n",
       "      <td>Mujeres de 12 a 14 de edad, nunca embarazadas</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>000102801</td>\n",
       "      <td>000102801  2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>010101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154803.0</td>\n",
       "      <td>Rico</td>\n",
       "      <td>1.295611</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>000102801</td>\n",
       "      <td>000102801  6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>010101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154803.0</td>\n",
       "      <td>Rico</td>\n",
       "      <td>1.295611</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>000104801</td>\n",
       "      <td>000104801  2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1434.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>010101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>154803.0</td>\n",
       "      <td>Pobrer</td>\n",
       "      <td>-0.256431</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38330</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>325406201</td>\n",
       "      <td>325406201  2</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>250401</td>\n",
       "      <td>249.0</td>\n",
       "      <td>244995.0</td>\n",
       "      <td>El m√°s pobre</td>\n",
       "      <td>-1.750187</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>15783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38331</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>325406301</td>\n",
       "      <td>325406301  2</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>250401</td>\n",
       "      <td>249.0</td>\n",
       "      <td>244995.0</td>\n",
       "      <td>El m√°s pobre</td>\n",
       "      <td>-1.676861</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>15783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38332</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>325407001</td>\n",
       "      <td>325407001  2</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>250401</td>\n",
       "      <td>249.0</td>\n",
       "      <td>459792.0</td>\n",
       "      <td>El m√°s pobre</td>\n",
       "      <td>-1.585333</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>15783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38333</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>325407201</td>\n",
       "      <td>325407201  2</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>250401</td>\n",
       "      <td>249.0</td>\n",
       "      <td>459792.0</td>\n",
       "      <td>El m√°s pobre</td>\n",
       "      <td>-1.650159</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>15783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38334</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>325407401</td>\n",
       "      <td>325407401  2</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3254.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>250401</td>\n",
       "      <td>249.0</td>\n",
       "      <td>244995.0</td>\n",
       "      <td>El m√°s pobre</td>\n",
       "      <td>-1.644720</td>\n",
       "      <td>Mujeres de 15 a 49 a√±os de edad</td>\n",
       "      <td>15783.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38335 rows √ó 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID1             HHID              CASEID    V001  V002  V003  \\\n",
       "0      2019.0        000100201        000100201  2     1.0   2.0   2.0   \n",
       "1      2019.0        000100201        000100201  3     1.0   2.0   3.0   \n",
       "2      2019.0        000102801        000102801  2     1.0  28.0   2.0   \n",
       "3      2019.0        000102801        000102801  6     1.0  28.0   6.0   \n",
       "4      2019.0        000104801        000104801  2     1.0  48.0   2.0   \n",
       "...       ...              ...                 ...     ...   ...   ...   \n",
       "38330  2019.0        325406201        325406201  2  3254.0  62.0   2.0   \n",
       "38331  2019.0        325406301        325406301  2  3254.0  63.0   2.0   \n",
       "38332  2019.0        325407001        325407001  2  3254.0  70.0   2.0   \n",
       "38333  2019.0        325407201        325407201  2  3254.0  72.0   2.0   \n",
       "38334  2019.0        325407401        325407401  2  3254.0  74.0   2.0   \n",
       "\n",
       "         V004    V007    V008  V009  ...  QD333_4  QD333_5  QD333_6  UBIGEO  \\\n",
       "0         1.0  2019.0  1434.0   4.0  ...       No       No       No  010101   \n",
       "1         1.0  2019.0  1434.0   1.0  ...       No       No       No  010101   \n",
       "2         1.0  2019.0  1434.0   6.0  ...       No       No       No  010101   \n",
       "3         1.0  2019.0  1434.0   3.0  ...       No       No       No  010101   \n",
       "4         1.0  2019.0  1434.0   5.0  ...       No       No       No  010101   \n",
       "...       ...     ...     ...   ...  ...      ...      ...      ...     ...   \n",
       "38330  3254.0  2019.0  1440.0  12.0  ...       No       No       No  250401   \n",
       "38331  3254.0  2019.0  1440.0   6.0  ...       No       No       No  250401   \n",
       "38332  3254.0  2019.0  1440.0   7.0  ...       No       No       No  250401   \n",
       "38333  3254.0  2019.0  1440.0  12.0  ...       No       No       No  250401   \n",
       "38334  3254.0  2019.0  1440.0  10.0  ...       No       No       No  250401   \n",
       "\n",
       "        V022      V005          V190      V191  \\\n",
       "0        3.0  154803.0          Rico  1.234450   \n",
       "1        3.0  154803.0          Rico  1.234450   \n",
       "2        3.0  154803.0          Rico  1.295611   \n",
       "3        3.0  154803.0          Rico  1.295611   \n",
       "4        3.0  154803.0        Pobrer -0.256431   \n",
       "...      ...       ...           ...       ...   \n",
       "38330  249.0  244995.0  El m√°s pobre -1.750187   \n",
       "38331  249.0  244995.0  El m√°s pobre -1.676861   \n",
       "38332  249.0  459792.0  El m√°s pobre -1.585333   \n",
       "38333  249.0  459792.0  El m√°s pobre -1.650159   \n",
       "38334  249.0  244995.0  El m√°s pobre -1.644720   \n",
       "\n",
       "                                        mujeres12a49  NCONGLOME  \n",
       "0                    Mujeres de 15 a 49 a√±os de edad     7065.0  \n",
       "1      Mujeres de 12 a 14 de edad, nunca embarazadas     7065.0  \n",
       "2                    Mujeres de 15 a 49 a√±os de edad     7065.0  \n",
       "3                    Mujeres de 15 a 49 a√±os de edad     7065.0  \n",
       "4                    Mujeres de 15 a 49 a√±os de edad     7065.0  \n",
       "...                                              ...        ...  \n",
       "38330                Mujeres de 15 a 49 a√±os de edad    15783.0  \n",
       "38331                Mujeres de 15 a 49 a√±os de edad    15783.0  \n",
       "38332                Mujeres de 15 a 49 a√±os de edad    15783.0  \n",
       "38333                Mujeres de 15 a 49 a√±os de edad    15783.0  \n",
       "38334                Mujeres de 15 a 49 a√±os de edad    15783.0  \n",
       "\n",
       "[38335 rows x 105 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mis_unique\u001b[49m(CASEID)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'is_unique' is not defined"
     ]
    }
   ],
   "source": [
    "is_unique(CASEID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Check if CASEID identifies each observation uniquely. Hint: Use is_unique method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Make the CASEID column the index of your data set. Hint: Use set_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Keep women who are 15-30 years old and live in Urban areas. Hint: Use query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Generate a new column with the month of born. Just use the first three letters. \n",
    "# Use the English names of the months."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
